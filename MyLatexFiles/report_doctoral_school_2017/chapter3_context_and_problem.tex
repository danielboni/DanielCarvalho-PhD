\chapter{Current Research Results}
\label{chap:context}
\minitoc

\section{Context and motivation}

User-guided data integration implies consuming data by matching and composing different data services, and integrating the results while respecting data consumers quality requirements. 
%
These requirements could be defined in service level agreement (SLA) contracts 
established between data consumers and data providers.
%
The SLA defines what a data consumer can expect as system behavior, 
but also the properties of the data such as its provenance, veracity, freshness, whether the consumer accepts to pay for data, and how much is he/she ready to pay for the resources necessary for integrating his/her expected result. 

%
Several authors have introduced algorithms for data integration in service-oriented architectures (such as~\cite{Barhamgi2010,Umberto,Benouaret2011,ba2014}).
%
They have focused on the query rewriting problem, which in this context includes matching, selecting and composing services
according to some data requirements and constraints. 
%
The problem of composing services has been proven to be an NP-hard problem which implies a performance problem while searching for service combinations.
%
To tackle it, heuristics for computing and identifying the best services and compositions based on quality aspects have been proposed (such as~\cite{Cardoso2004,Berbner2006,Menasce:2008,Sasikaladevi2014}).
%
However, current work focus on services' properties neglecting data properties and the new constraints imposed by the service-oriented context (for example, a given \textsl{data service} could be out of resources according to what he agreed on  SLA with his \textsl{service provider}). 

%
Thus, the objective of this work is to address data integration targeting the multi-cloud context. 
%
The originality of our approach consists in guiding the entire data integration solution taking into consideration (i) data consumer requirement statements with respect to data services performance properties (for instance, availability and response time) and to data collection properties (provenance, freshness, veracity, data type, among others); (ii) infrastructure properties (reliability, computing, storage and memory capacity, and cost) imposed by the service-oriented context; and (iii) SLA contracts exported by different data services and service providers.
%
%Combining data from different data sources (known in advance) and providing 
%a unified view of the result to data consumers is a well-known problem in 
%database theory called data integration. 
%
%Many authors have proposed algorithms for rewriting a data consumer query
%according to different views expressed over different database schemas.
%
%In general, these approaches are expensive in terms of computing resources
%sharing the 
%same performance problem while combining the different query concepts 
%in order to produce a rewrite according to the expectations. 

%Data integration solutions could take advantages from cloud architectures.
%%
%Cloud providers deliver on-demand computing resources to cloud consumers
%(for example, an end-user, a data service or another cloud)
%according to the quality of service they have agreed in a service level 
%agreement (SLA) contract. 
%%
%An SLA states what the data consumer can expect from a service or a system
%behavior.
%
%Data integration can be seen in the cloud computing as a service matching 
%and composition problem which implies consuming data from different data 
%services and integrating results. 
%%
%Researches in this domain also deal with problema de processamento ...have proposed 

%\bigskip
%Data services and data processing services can be deployed in cloud infrastructures under different quality measures. Such measures describe the conditions in which a service can provide or process data. These measures can be expressed in a service level agreement (SLA), which states what the user can expect from a service or system behavior.
%However, the current SLAs are mainly interested in performance aspects (such response time, availability, memory and others) putting aside data quality requirements (such as freshness, veracity, type and others).

%Data integration problem is widely studied in the database domain~\cite{Lenzerini:2002}. 
%Current data integration implies consuming data from different data services and integrating the results while meeting users' quality requirements. Such requirements include the data that is retrieved and integrated, but also the properties of the data, its producers and the conditions in which such data is produced and processed. For example, whether the user accepts to pay for data, its provenance, veracity and freshness and how much is the user ready to pay for the resources necessary for integrating her expected result. 

%Data services and data processing services can be deployed in cloud infrastructures under different quality measures. Such measures describe the conditions in which a service can provide or process data. These measures can be expressed in a service level agreement (SLA), which states what the user can expect from a service or system behavior.
%For example, whether it implements an authentication process, if it respects data consumer's privacy and the quality of the data the service can deliver, like freshness, veracity, reputation and other non-functional conditions like the business model that controls data delivery. 
%However, the current SLAs are mainly interested in performance aspects (such response time, availability, memory and others) putting aside data quality requirements (such as freshness, veracity, type and others).

%Data integration can be seen in the cloud computing as a service composition problem. 
%In our previous work~\cite{Carvalho2015}, we have identified that QoS aspects has started to be considered while integrating data, and the cloud has become a popular environment to perform data integration. Moreover, data integration combined with SLA is an open issue in the cloud. 
%In this sense, the objective of this work is to address data integration in a multi-cloud context. 
%The originality of our approach consists in guiding the entire data integration solution taking into consideration
%(i) user preferences statements (regarding data quality requirements); 
%(ii) infrastructure properties (reliability, computing, storage and memory capacity, and cost) imposed by the multi-cloud context; 
%(iii) SLA contracts exported by different cloud providers; and 
%(iv) several QoS measures associated to data collections properties (for instance, trust, privacy, economic cost).

%Thus, the \textbf{first challenge} is to compute what we call an integrated SLA that matches the user's integration preferences (including quality constraints and data requirements) with the SLA's provided by cloud services, given a specific user cloud subscription. The user may have general preferences depending on the context she wants to integrate her data such as economic cost, bandwidth limit, free services, and storage and processing limits. The SLA's associated to the cloud services can be of different types: user - data service, data service - cloud provider, data provision service - data processing service, and cloud provider - cloud provider. In this context, matching the user integration preferences with the services that can contribute to produce a result can lead to search and identify in the chain of SLAs. Probably it is possible to find an incompatibility between the preferences and a SLA in the chain, in this case it is necessary to propose a strategy to solve the problem. 
%
%Furthermore, in order to fulfill requirements and satisfy user expectations, it is possible to have a collaboration between different clouds. This collaboration implies the agreement through SLAs between services deployed in different cloud providers. In consequence, matching user preferences with SLA's can lead to deal with heterogeneous SLA specifications (different schemata, different measures semantics and granularities). Computing an integrated SLA can imply dealing with heterogeneous SLA specifications and SLA-preferences incompatibilities.

%The \textbf{second challenge} is to guide data integration taking into consideration the integrated SLA. Here, the data integration process includes (i) looking up services that can be used as data providers, and for services required to process retrieved data and build an integrated result; (ii) performing data retrieval, processing and integration and (iii) deliver results to the user considering her preferences (quality requirements, context and resources consumption). The integrated SLA can guide services filtering in the look up phase; it can help to control the amounts of data to retrieve and process according to consumption rights depending on the user subscription to the participating cloud providers and how to deliver data considering the user's context.

\section{Problem statement}\label{problem}
%
The service-oriented context brings challenges to query rewriting in data integration solutions.
%
Instead of taking into consideration only the user query and his/her requirements with respect to services' properties (such as percentage of availability and response time), the integration process must consider in addition the new constraints imposed by the context: 
%
\begin{itemize}
\renewcommand{\labelitemi}{$-$}
\item User requirements may concern not only data services' properties but also quality requirements of the data which is being provided (such as freshness, cost, provenance, data type, veracity among others).
%
\item Data provision is constrained to the available computing resources agreed between data services and service providers on service level agreement (SLA) contracts. For example, a data service could have agreed to perform a limited quantity of requests per day.
%
\item The data integration process requires a high level of computing resources while searching for services and producing compositions.
The huge amount of data and data services in the service context increases even 
more the complexity of the solutions.
\end{itemize}
%
With regards to the aforementioned, current data integration solutions introduce a multi-dimensional matching problem that should take into account:
%
\begin{itemize}
\renewcommand{\labelitemi}{$-$}
%\item Expressing data consumer requirements with respect to performance capabilities of data services and to the quality of the expected data.
%
\item Matching and selecting data services according to the data consumer requirements (concerning service and data's properties) with respect \textit{(i)} to data 
services quality measures defined in SLA contracts; and \textit{(ii)} to data services' available resources according to different SLAs that they have agreed with different service providers.
%
%\item Matching and selecting data services which have available resources according
%to different SLAs that they have agreed with different cloud providers.
%
\item Delivering results with respect to the data consumer requirements depending on the context which he/she consumes the data (for example, using a mobile phone).
\end{itemize}
%
Thus, this thesis addresses data integration on service-oriented environments.
%
The aim is to propose a data integration approach targeting (multi-)cloud context in 
which data is delivered according to data consumers expectations and financial constraints by profiting from 
previous integration results instead of launching entirely the expensive data integration 
process. 


\section{State of the Art}
%
The state of the art comprehends four groups, which are discussed in the following sections: 
\textit{(i)} data integration approaches in service-oriented contexts;
\textit{(ii)} query rewriting approaches;
\textit{(iii)} data integration and data quality in the database domain; and \textit{(iv)} service level agreements.

\subsection{Data integration in service-oriented domains}

The data integration is a well-known problem in the database domain which intends to offer to data consumers an integrated view of data 
build by combining and merging several databases \textit{known-in-advance}. 
This problem has also been addressed in cloud and service-oriented contexts.
%
For instance, \cite{YauY08} introduced a repository in which based on user 
integration requirements, the repository retrieves and integrates the data
collected from different services focusing on data privacy issues. 
%
\cite{Tian2010} proposed an inter-cloud data integration system which considers 
privacy requirements and the cost for protecting and processing data. 
According to the user privacy requirements, the system decides where is the best
location to execute the query while meeting privacy and cost constraints:
direct on service providers or on its own cloud repository.
%
\cite{benslimane2013} designed a composition-based approach for data integration
taking into consideration the privacy of the data manipulated by web services and 
compositions.
%
\cite{YauY08,Tian2010,benslimane2013} considered privacy and cost while integrating data, but 
other quality aspects associated to data itself (such as veracity, type, freshness,
among others) and to the infrastructure are still missing.
%

Some authors have addressed data integration in service-oriented contexts
taking into consideration the requirement of computing resources for integrating data
across several data service.
%
For instance, \cite{Correndo2010} presented a method for data integration using SPARQL
on Linked Data. The objective is to solve the entity co-reference problem 
and to exploit ontology alignments interested in data manipulation. 
%
\cite{Thor:2011} introduced an mashup-based integration system called \textit{CloudFuice}.
It provides a language for specifying dataflows for performing parallel data integration
tasks.	
%
\cite{Alsubaiee:2012} introduced a system which allows to store, consume, integrate and
analyze social and wed data in a scalable manner.
\cite{Mubeen2012} proposed a new architecture for web-scale data integration in which
the data extracted from services are stored in a single cloud data store.
%
\cite{ElSheikh2013} designed the \textit{SODIM} which combines data integration,
service oriented architecture and distributed processing applying MapReduce techniques. 
The system works on a pool of collaborative services and it can process a large number 
of databases represented as web services. 
\cite{Hong2014} introduced a cloud data service system to integrate data from distributed
data services considering three level of data security focusing on data privacy.
In general, \cite{Correndo2010,Thor:2011,Alsubaiee:2012,Mubeen2012,ElSheikh2013,Hong2014}
exploited parallel settings for implementation costly data integration processes.

\subsection{Query rewriting approaches}
In traditional databases theory, query rewriting activities are essential to 
data integration solutions. In the service-oriented context query rewriting issues
are commonly referred as a service matching and composition problem in which given a query, 
the objective is to match and compose services that can contribute to produce a result. 
\cite{Barhamgi2010} proposed a query rewriting approach which processes queries on data provider services.
The query and data services are modeled as RDF views. Then, a rewriting answer is a 
service composition in which the set of data service graphs fully satisfy the query graph.  
%
Inspired by~\cite{Barhamgi2010}, \cite{Benouaret2011} introduced a service composition 
framework to answer preference queries which ranks the compositions based on previously 
computed scores.
%
\cite{Umberto} presented a refinement approach of web service compositions in which
given an abstract query specification, a set of concrete service compositions are produced.
%
\cite{ba2014} extended \cite{Umberto} introducing the notion of user preferences and scores 
used to rank services and compositions while rewriting the query specification.
%
As in the database domain, the algorithms discussed above deal with performance issues
while rewriting depending on complexity of the query and on the number of available services.
Although \cite{Benouaret2011,ba2014} have considered preferences and scores to produce rewritings, the service-oriented context introduces new requirements and constraints to the integration process. Currently, the approaches are not sufficient to cover the new challenges. Thus, they should be revisited and adapted in order to make the integration efficient in this environment. 

%As traditional databases theory, data integration on cloud and service-oriented context deals with query rewriting issues. Existing works like~\cite{ba2014,Barhamgi2010,Benouaret2011,Umberto} have refered it as a service composition problem. Given a query, the objective is to lookup and compose data services that can contribute to produce a result. In general, these works must address performance issues, because they use algorithms that can become expensive according to the complexity of the query and on the number of available services. Although \cite{ba2014,Benouaret2011} have considered preferences and scores to produce rewritings, the multi-cloud context introduces new requirements and constraints to the integration process. Currently, the approaches are not sufficient to cover the new challenges. Thus, they should be revisited and adapted in order to make the integration efficient in this new environment. 

%In general, these approaches share the same performance problem as the traditional database algorithms. Furthermore, they do not take into consideration user's integration requirements which can lead to produce rewritings that are not satisfactory to the user in terms of quality requirements and cost.

\subsection{Data integration and data quality in the database domain}
 
Data integration has been widely examined in the database theory.
\cite{Lenzerini:2002} discussed theoretical aspects in data integration including modeling 
applications, query evaluation, dealing with inconsistencies and reasoning queries.
The core of data integration is the rewriting process.
Many authors have reported algorithms for this purpose. 
For instance, \cite{Levy:1996} proposed the \textit{bucket algorithm}, which generates 
combinations of the different \textit{buckets} (views mapped to a given subgoal), and checks 
whether each one is a rewriting of the query. 
\cite{Duschka:1997} introduced the \textit{inverse-rules algorithm}, which produces a set of
\textit{inverse rules} from local views to the global view. Then, a rewriting is obtained by 
unfolding the query in terms of the \textit{inverse rules}. 
\cite{Pottinger:2001} presented the \textit{MiniCon algorithm} funded in ideas 
of~\cite{Duschka:1997}.
Although \textit{MiniCon} has shown a more efficient implementation, these kind of algorithm
share the same performance problem while combining views to produce a rewriting.
Several query rewriting approaches were reviewed in~\cite{Halevy:2001}.

Data quality issues in data integration have been explored in many researches. 
\cite{Gertz1998} designed a framework for modeling quality aspects
(such as timeliness, accuracy and completeness) in databases integration. The framework
stores quality aspects as meta-data in the integration level, and based on this information
delivers a high-quality data from local databases.
\cite{Scannapieco:2004} presented a data quality broker that allows to submit 
queries with associated quality requirements over a global schema and to 
provide results according to them.
\cite{Batista07} proposed a quality criteria (such as reputation, availability, 
response time, completeness, among others) analysis of data integration elements 
(sources, schemas and data) to improve quality on query execution. 
\cite{Moneim2015} proposed a method to improve quality in data integration solutions
by adding and storing a set of data quality measures in data sources. Then, given a 
user query and quality preferences, the most relevant sources are selected to answer
the query.
\cite{Monem2016} introduced a framework (called DIRA), which produces the \textit{top-k}
query answers using an algorithm that selects data sources based on quality aspects stored
as meta-data in a data source such as accuracy, validity, completeness, among others. 
Others data integration issues and quality aspects in data integration systems are tackled
in~\cite{Batini2006,Angeles2009,Boufares2012}. 

\subsection{Service Level Agreement}
Service level agreements (SLA) state what a \textsl{consumer} can expect from a system or 
system behavior.
Several researchers have reported studies on SLAs in different domains~\cite{AlhamadDC11}.
On cloud computing, contributions associated to SLAs manly comprehend: \textit{(i)} management, 
negotiation and matching of SLAs; \textit{(ii)} security issues; and \textit{(iii)} resource allocation. 

For instance, \cite{Redl2012} introduced a method for matching of SLA elements exported by different cloud providers.
%
\cite{Mavrogeorgi2013} presented a SLA management framework responsible to negotiate and enforce
customized SLAs enriched with the information of the cloud federation and renegotiation rules.
%
\cite{Son2014} introduced SLA negotiation strategies for a multi-cloud broker and discussed
design issues.
%
\cite{Falasi2016} designed an SLA negotiation model to based on the game-theory to allow cloud
services to specify, negotiate and establish SLAs.
%  

Other proposals intends to assure security aspects in the cloud by defining and including
security requirements on contracts. For instance, \cite{rak2013} introduced an approach to specify,
assess and integrate security requirements to cloud services. 
%
\cite{Rojas2016} proposed a framework for management of SLA security requirements and enforcing them
during the entire SLA lifecycle.
%
\cite{Casola2016} specified a catalogue of security services that can be monitored and negotiated through SLAs.

Finally, several works address SLA monitoring to detect and avoid violations.
For example, \cite{Brandic2010} designed a model to identify and propagate
SLA violation to the cloud providers. 
\cite{Leitner2010} introduced a event-based system to predict SLA violations
and take necessary measures before they have impacted the provider SLA.
\cite{Emeakaroha2012} proposed a system to monitor SLA in order to detect violation
and provide tools for resource allocation and scheduling. 
\cite{Hussain2015} described a profile-based SLA prediction model, which predicts
the necessary resources based on user reputation history.
 
To the best of our knowledge, related work on SLA models does not seem to 
address data integration in service-oriented and (multi-)cloud architectures. Probably, because the 
contributions are mainly interested in specifying and monitoring performance 
aspects (for example, response time, memory, availability, among others) in the SLA 
rather than quality issues regarding the data properties (such as data type, 
veracity, freshness, provenance and others).
Thus, we strongly believe that SLAs can be used to explicitly introduce the 
notion of quality in the current data integration solutions. 
In this sense, the use of SLAs to guide the entire data integration in a 
service-oriented context seems original and promising for providing new perspectives to the data integration problem.

% In the cloud context, Rak \textit{et al.} proposed an approach to specify security requirement and to associate them to cloud services~\cite{rak2013}. Mavrogeorgi \textit{et al.} introduced a SLA management framework that allows the creation and enforcement of customized SLAs~\cite{Mavrogeorgi2013}. Leitner \textit{et al.} presented an approach to monitor and predict SLA violations before they impacted the provider's SLA~\cite{Leitner2010}. In general, proposals regarding SLAs in the deployment of services focus on two aspects: (\textit{i}) approaches focusing on the life cycle of the SLA mainly interested in the contract negotiation phase between the cloud and the service consumer; and (\textit{ii}) works monitoring contracts and cloud resources in order to avoid SLA violations, and consequently penalties due to its violation. In this sense, to the best of our knowledge, we have not identified any other approach that proposes the use of SLA associated to a data integration solution in a multi-cloud environment.



\section{Results and Contribution}

In order to address our research problem (see section~\ref{problem}), we have proposed data integration approach in which given a query and a set of user requirements associated to it, the query execution process is divided in three phases. The figure~\ref{approach} illustrates our approach.


%During the first year of project, we have been working on the state of the art. The idea is to be aware of all types of publications close related to the thesis proposal. To reach this, we proceeded with a literature analysis using a systematic mapping methodology. 
	
%Briefly, the methodology consists in retrieving papers from scientific databases using the same search string. 
%These papers are filtered according to an inclusion and exclusion criteria that should be defined based on the research interests. 
%The papers will be classified in different categories (called facets) and for each facet in a specific dimension. 
%The facets and dimension are defined based on the authors' knowledge and interests. 
%Taking the final papers collection, the abstracts should be read in order to classify each paper into the dimensions for each facet. 
%This methodology allowed us to identify trends and open issues regarding our research topic and proposing an approach that fills some gaps and proposes an original data integration solution according to current trends in the area.
%The table~\ref{table:sysmap} presents our results. We retrieved 1832 papers. 114 papers were selected based on our inclusion and exclusion criteria.
%As result of this work, we have published a paper on DEXA 2015 (See annex~\ref{chap:appendix1}).

%\begin{table}
%\center
%\begin{tabular}{|c|c|c|c|}
%\hline 
%\textbf{Database} & \textbf{Amount} & \textbf{Include} & \textbf{Excluded} \\ 
%\hline 
%\textit{IEEE} & 658 & 56 & 602 \\ 
%\hline 
%\textit{ACM} & 649 & 31 & 618 \\ 
%\hline 
%\textit{Science Direct} & 106 & 6 & 100 \\ 
%\hline 
%\textit{CiteSeerX} & 419 & 21 & 398 \\ 
%\hline 
%\textit{Total} & 1832 & \underline{114} & 1718 \\ 
%\hline 
%\end{tabular} 
%\caption{Systematic Mapping: sumary of paper per databse.}\label{table:sysmap}
%\end{table} 

% \subsection{Second year}

%During the second year, we have been working on the development of our data integration approach. Given a query and a set of user preferences associated to it, the query execution process is divided in three phases. The figure 1 illustrates our data integration approach.

\begin{figure}[h!]
\center
\includegraphics[scale=0.7]{/images/general_approach.PNG} 
\caption{SLA-guided data integration approach}\label{approach}
\end{figure}

\underline{The first phase} is the SLA derivation in which a SLA for the user request is created. It consists in looking for a (stored, integrated) SLA derived for a similar request in our history. If a similar SLA is found, the request is forwarded to the query evaluation phase. Otherwise, a new SLA to the integration (called integrated SLA) is produced. 
%The query is expressed as a service composition with associated user preferences. 
In \underline{the second phase}, service composition, the query is rewritten in terms of different \textsl{data services} considering the user requirements (according to service properties and data properties) and the SLAs of each service involved in the composition. The rewriting result is stored in the history for further uses. \underline{Finally}, in the query evaluation phase, the query is optimized in terms of user preferences and SLAs concerning the consumed resources and the economic cost of the query. Once optimized, the query processed in the execution engine. In addition, we are assuming a SLA management module and monitoring system responsible to verify if the SLA contracts are being respected. 
%Firstly, we have worked on the phases two in order to have an algorithm that will allow us to run important experiments to evaluate our approach. 
%The following activities were developed to achieve our objectives:

Following the contributions of this work are presented. Note that this work was built based on a set of papers collected by applying the systematic mapping methodology. This exercise allowed us to identify new trends and open issues concerning data integration and SLA. As result, we have published a paper on DEXA 2015 (See appendix~\ref{chap:appendix1}).

	
\bigskip
\noindent \textbf{Service-based query rewriting algorithm:}
Based on the related work, we have developed and formalized the \textit{Rhone} service-based query rewriting algorithm guided by service level agreements (SLA). The \textit{Rhone} assumes that there are a set of quality measures associated to services which we suppose they are previously extracted from their SLA. These measures will guide the service selection and the entire rewriting process.  Our work address this issue and proposes the algorithm with two original aspects: (i) the user can express his/her quality preferences and associated them to his/her query; and (ii) service's quality aspects defined in SLAs guide the service selection and the whole rewriting process taking into consideration that services and rewritings should meet the user requirements, and the different cases of incompatibilities of SLAs, uncompleted SLA and the integration SLA. 
%and (ii) while trying to fulfill the user quality preferences, service’s quality aspects defined in SLAs guide the service selection and the whole rewriting process (Services and rewritings should meet the user requirements).
Given a set of abstract services, a set of concrete services and a user query (both defined in terms of abstract services), and a set of user quality preferences, the \textit{Rhone} derives a set of service compositions that answer the query and that fulfill the quality preferences regarding the services and datas' properties. The algorithm consists in four steps: (i) \textit{Selecting concrete services}. Similar to~\cite{Levy:1996,Pottinger:2001} our algorithm selects services based on the abstract services that exists in the query, but it includes two differences: first, a concrete service cannot be selected if it contains an abstract service that is not present in the query; and second, the service' quality aspects (extracted from its SLA) must be in accordance with the user quality preferences; (ii) \textit{creating mappings from concrete services to the query (called concrete service description (CSD))} inspired in~\cite{Pottinger:2001} including also the information concerning the services' SLA; (iii) \textit{combining CSDs}; and (iv) \textit{producing rewritings} until fulfilling the user requirements according to the services' SLA. Each phase of the algorithm and each concept (query, concrete services, mapping rules, for instance) were formally specified and described. The results of this part of our work have been published on ADBIS 2016 (See appendix~\ref{chap:appendix2}).


%
%\cite{ba2014} have proposed an algorithm for refining services composition. Their goal was to use user preferences to select and rank services in order to avoid the exponential problem while combining and producing compositions. Composition are incrementally produced until to reach a (predefined) desired number. In collaboration with our colleagues in Brazil (authors in~\cite{ba2014}), we have worked on an adapted version of \cite{ba2014} to our data integration solution extending their data structure to map services to the query, and adding the concepts of user preferences to the query and quality measures to the services. However, while performing the integration we have identified some design issues that made it useless to our approach: (i) their concept of user preferences are scores associated to services previous defined by the user while, for us, user preferences are quality requirements expected by the user concerning the whole integration; (ii) their algorithm accepts rewriting including calls to services that are not interest for the composition. Assuming that on the cloud each service has a price associated to its request, these composition that calls useless services produces an extra cost to the user. This adaptation process helped us to identify important issues to be applied to our own implementation such as developing an better approach to produce the combinations of services.
%
%
%\cite{ba2014} have proposed an algorithm for refining services composition. Their goal was to use user preferences to select and rank services in order to avoid the exponential problem while combining and producing compositions. Composition are incrementally produced until to reach a (predefined) desired number. In collaboration with our colleagues in Brazil (authors in~\cite{ba2014}), we have worked on an adapted version of \cite{ba2014} to our data integration solution extending their data structure to map services to the query, and adding the concepts of user preferences to the query and quality measures to the services. However, while performing the integration we have identified some design issues that made it useless to our approach: (i) their concept of user preferences are scores associated to services previous defined by the user while, for us, user preferences are quality requirements expected by the user concerning the whole integration; (ii) their algorithm accepts rewriting including calls to services that are not interest for the composition. Assuming that on the cloud each service has a price associated to its request, these composition that calls useless services produces an extra cost to the user. This adaptation process helped us to identify important issues to be applied to our own implementation such as developing an better approach to produce the combinations of services.

%a) Adapting a previous query rewriting algorithm to our approach. This work was performed in collaboration with our colleagues in Brazil. The idea was to adapt a previous work from their lab to our data integration solution. While integrating we have identified some design problems that made it useless to our approach. However, the work in group help us to identify important issues to be applied to our own implementation.

%b) Query rewriting algorithms. We have produced a state of the art about query rewriting algorithm.  The approaches are divided into two different domains: database and service-oriented architecture. In the database domain, the different works concerning query rewriting using views have been studied in the literature. In general, the approaches deal with an exponential problem depending on the size of the query and the quantity of views. In the service-oriented domain, the approaches share the same problem. Producing services composition is extremely costly depending on the size of the composition (query) and the amount of services to be combined. Some approaches have tackled this issue. They have tried to minimize the effort to produce rewritings by considering user preferences or by limiting the desired number of rewritings. These works have been used as base and reference to produce our own algorithm.

%Starting from the knowledge acquired while working on~\cite{ba2014}, we have produced a state of the art about query rewriting algorithm. The approaches can be divided into two domains: database and service-oriented architecture. In the database domain, studies (such as \cite{Duschka:1997,Levy:1996,Pottinger:2001}) have documented their approaches for answering queries using views. In general, while producing rewritings, these approaches suffer from the time exponential problem depending on the size of the query and the quantity of views. In the service-oriented domain, query rewriting can be seen as a service composition problem. Approaches (such as \cite{Barhamgi2010,Benouaret2011,Umberto}) share the same problem as on the database domain while producing services composition which is a task extremely costly depending on the size of the composition (query) and the amount of services to be combined. \cite{ba2014} have tried to minimize the effort to produce rewritings by considering user preferences (as scores) and by limiting the desired number of rewritings.

%Based on the related work, we have developed and formalized the \textit{Rhone} service-based query rewriting algorithm guided by service level agreements (SLA). The \textit{Rhone} assumes that there are a set of quality measures associated to services which we suppose they are previously extract from their SLA. These measures will guide the service selection and the entire rewriting process.  Our work address this issue and proposes the algorithm with two original aspects: (i) the user can express her quality preferences and associated them to his query; and (ii) service's quality aspects defined in SLAs guide the service selection and the whole rewriting process taking into consideration that services and rewritings should meet the user requirements, and the different cases of incompatibilities of SLAs, uncompleted SLA and the integration SLA. 
%%and (ii) while trying to fulfill the user quality preferences, service’s quality aspects defined in SLAs guide the service selection and the whole rewriting process (Services and rewritings should meet the user requirements).
%Given a set of abstract services, a set of concrete services and a user query (both defined in terms of abstract services), and a set of user quality preferences, the \textit{Rhone} derives a set of service compositions that answer the query and that fulfill the quality preferences regarding the context of data service deployment. The algorithm consists in four steps: (i) \textit{Selecting concrete services}. Similar to~\cite{Levy:1996,Pottinger:2001} our algorithm selects services based on the abstract services that exists in the query, but it includes two differences: first, a concrete service cannot be select if it contains an abstract service that is not present in the query; and second, the service' quality aspects (extracted from its SLA) must be in accordance with the user quality preferences; (ii) \textit{creating mappings from concrete services to the query (called concrete service description (CSD))} inspired in~\cite{Pottinger:2001} including also the information concerning the services' SLA; (iii) \textit{combining CSDs}; and (iv) \textit{producing rewritings} until fulfilling the user requirements according to the services' SLA. Each phase of the algorithm and each concept (query, concrete services, mapping rules, for instance) were formally specified and described. As result to this work, we have published a paper on ADBIS 2016 (See annex~\ref{chap:appendix2}).

%d) Configuration of a multi-cloud environment. We have worked on the configuration of a multi-cloud environment. However, we found some problems at this point: (i) the configuration and deployment of cloud infrastructure using open source technologies is not easy and requires important technical skills while configuring the network resources; (ii) it requires a powerful machine. Due to that we have configured a simulation of cloud; and (iii) we have searched for private cloud providers, but they allow few access and power permissions to manage resources. In addition, they are quite expensive. We are still working on best way to manage this issue.

\bigskip
\noindent \textbf{Cloud infrastructure:}
In order to evaluate our approach and the \textit{Rhone} algorithm, we began configuring a multi-cloud environment. We have searched for open source solutions instead of privates once they are (i) quite expensive; and (ii) do not allow to extend and access directly the different level of SLAs. The OpenStack was selected as our technology. We have installed and configured the different modules necessaries to the OpenStack. However, we have some issues: (i) the configuration and deployment of cloud infrastructure require important technical skills while configuring the network resources; (ii) it requires a powerful machine. Due to these reasons we have configured a simulation of cloud run our experiments.


\bigskip
\noindent \textbf{Algorithm implementation and evaluation:}
\textit{Rhone} was implemented using Java according to its formal definition. The algorithm was tested in a cloud simulation containing 100 services in its service registry. We have tested different types of query varying on size and on number of user preferences. Although our algorithm shares the same time performance problem as the previous approaches while combining compositions, the experiments have shown that the \textit{Rhone} can enhance the quality and reduce the cost in data integration by considering the user preferences and service's quality aspects extracted from service level agreements.
The results can be found in appendix~\ref{chap:appendix2}.
 

%We developed an improved version of the algorithm that better manages the manner in which lists of objects are managed. We have applied this version to a new set of experiments running 100 concrete services. With the results obtained from this experiment and the final version of our formalization, we are working on a new paper that included an extensive description of the algorithm and its evaluation to be submitted to ADBIS 2016 (deadline 27th March).

%h) SLA model to data integration. The state of the art on SLA have been analyzed to serve as basis to our SLA model. The works on SLA to cloud computing can be divided in two groups: (i) approaches dealing with the SLA negotiation phase. They focus on methods to stablish good and well-defined agreements between providers and customers; and (ii) works focus on monitoring/allocating resources in order to detect and avoid SLA violations. These works helped while proposing our SLA model and schema. Note that the SLA is the main concept in our proposal. It is responsible to guide the entire approach from the beginning to the end. There are some challenges regarding SLA: (i) once we are inserted into a multi-cloud context, we are dealing with a large heterogeneity of SLAs among different clouds. It is possible to have the same concept defined in a different way depending on the cloud; (ii) there are different levels of SLA: user SLAs, service SLAs and cloud SLAs. It is necessary to have a mapping between SLA measures that can be expressed in a different manner depending on the level; and (iii) it is possible to face a diversity chain of SLA from different levels and to map and identify measures is a hard process.

\bigskip
\noindent \textbf{Data integration meta-model and SLA schemas:}
In order to better describe our context to illustrate the approach, we have designed a meta-model for data integration. Current SLA schemas are focused on service properties aspects such as availability and response time. Thus, to better fit on the data integration requirements, we have proposed \textsl{cloud SLA} that is an agreement between a \textsl{data service} and a \textsl{cloud provider}, and a \textsl{service SLA} which is a new kind of agreement defined by \textsl{data services} exposing the properties of the data they provide. The results of this part of our work have been accepted in the ICSOC PhD Symposium 2016 (See appendix~\ref{chap:appendix3}).

%With this work performed, we have been designing our SLA model to data integration. As mentioned before, proposals to SLA in cloud computing can be divided in two groups: (i) approaches dealing with the SLA negotiation phase. They focus on methods to establish good and well-defined agreements between providers and customers; and (ii) works focus on monitoring/allocating resources in order to detect and avoid SLA violations. These works helped us while proposing our SLA model and schema. The SLA is the main concept in our proposal. It is responsible to guide the entire approach from the beginning to the end while fulfilling the user requirements. At this point, some challenges arises: (i) In a multi-cloud context, we are dealing with a large heterogeneity of SLAs among different clouds in terms of semantics and structure. A SLA and its measure can be defined in a different way depending on the cloud; (ii) there are different levels of SLA: between users and services, between services and clouds, between clouds and clouds. Consequently, a user requirement defined in a user SLA is computed in terms of different measures on Service and Cloud SLAs. It is necessary to have a mechanism that maps and compute the user requirement and SLA measures; and (iii) it is possible to exists a chain of SLA, and to map and computed measures in this chain can require a hard processing. i) Paper to ADBIS and VLDB PhD consortium. Currently, we have been working on a paper that focus on the description of our SLA model, schema and data integration approach to be submitted to the VLDB PhD workshop (deadline 4 April).

\bigskip
\noindent \textbf{A method for service and composition selection:}
Current data integration implies dealing with a huge number of \textsl{data services}. Consequently, query rewriting activities become more expensive in terms of computing resources and generation time. In this scenario, it is mandatory to develop a method to identify the best services to produce the best compositions that can achieve the user satisfaction. Thus, we have proposed an heuristic to rank \textsl{data services} and \textsl{compositions} based on SLA measures concerning  service properties (percentage of availability, response time, throughput and others) and data properties (data type, freshness, veracity, provenance and others).

\bigskip
\noindent \textbf{Definition and formalization of a taxonomy of queries:}
We have defined and formalized a set of possible relations between queries which differ in terms of \textsl{abstract services}, service properties and data properties.

\bigskip
\noindent \textbf{A method for reusing queries:}
It is well-known that query rewriting is expensive. Thus, based on the proposed query taxonomy, we have designed and formalized a reusability approach which allows to reuse \textsl{data services} and \textsl{compositions} from previous integration in order to profit from them.

\bigskip
\noindent \textbf{Query history data model and implementation:}
The reusability approach is based on a history of previous integrated queries. While defining the query taxonomy and reusability issues, we have identified the key information that should be part of the history allowing a satisfactory reusability process. A query history data model, which includes queries, abstract services, data services and compositions, was designed using the collected information. Further, the \textsl{Rhone} algorithm was adapted to be in accordance with the model.


\section{Perspectives}
The reusability approach required the specification of three algorithms to implement reusability functions. These algorithms are being introduced in \textit{Rhone}. Currently, we are in process of building the proof of concept to the approach: the query history was implemented in a relational database and we are running new experiments in order to compare them to the previous results. This work will result in a paper to the 36th International Conference on Conceptual Modeling (ER 2017) -- to be submitted in 17th April 2017.